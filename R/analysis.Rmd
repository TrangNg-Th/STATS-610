---
title: "Simulation-based model selection for population biology - STAT 610 - (paper results replication)"
author: Trang Nguyen

output:
  pdf_document:
    toc: true
    toc_depth: 4
---

# 1. Context : 
In this project, I will replicate the results from the paper "Simulation-based model selection for population biology" by Toni et al. (2009). 
The goal is to implement the simulation-based model selection approach described in the paper and apply it to a population biology scenario.

The scenario considered is about the spread of different strains of the influenza virus.
The data used in the paper comes from influenza A (H3N2) outbreaks that occurred in 1977-1978 and 1980-1981 in Tecomseh, Michigan, (Supplementary table 2) and a second dataset of an influenza B infection outbreak in 1975-1976 and influenza A (H1N1) infection outbreak in 1978-1979 in Seattle, Washington (Supplementary table 3).

# 2. Questions:
In the paper, the authors focused on two main questions regarding the influenza outbreaks.
1. Can different outbreaks (two periods) of the same strain be described by the same model of disease spread?
2. Can different outbreaks (two periods) of different strains be described by the same model of disease spread?

## Assumptions of the paper :
In the paper, the authors assumed that :
- The virus can be spread from the Infected individuals to the Susceptible individuals.
- The spread can occur both within households and across the population at large, and these spreads are different.


## Parameters to infer :
In the model, there are three key parameters :
- $q_c$ : the probability that a susceptible individual does not get infected from the community.
- $q_h$ : the probability that a susceptible individual escapes infection within their household.
- $w_{js}$ : the probability that $j$ out of the $s$ susceptibles in a household become infected, $$w_{js} = \binom{s}{j}w_{jj}(q_cq_h^{j})^{s-j}$$
where $w_{0s}=q_{cs}, s=0,1,2,…$ and $w_{jj}=1-\sum_{i=0}^{j-1}w_{ij}$.

We want to **infer** $q_c$ and $q_h$ using data from Supplementary table 2 and table 3.
In this paper, they considered two models :
- for supplementary table 2:  one with four parameters (one pair for each outbreak) and one with two parameters (shared between the two outbreaks).
- for supplementary table 3: one with four parameters (one pair for each outbreak) and one with three parameters (shared $q_h$ between the two outbreaks, but different $q_c$ for each outbreak).

### Predefined parameters :
- Number of particles N = 1000
- Number of replicates for each particle $B_t$ = 1 (deterministic simulation)
- Prior distributions of all parameters (hence $q_c$ and $q_h$) are chosen to be uniform over the range [0, 1].
- Prior distribution over models is uniform (truncated discrete uniform over the number of models M).
- The distance function used to compare simulated and observed data is defined as follows :
  $$ d(D_0, D^*) = \frac{1}{2} || D_1 - D^*(q_{h1}, q_{c1}) ||_F  + \frac{1}{2} || D_2 - D^*(q_{h2}, q_{c2}) ||_F $$
  $D_0 = D_1 \cup D_2$ is the observed data (the combination of the two outbreaks $D_1$ and $D_2$) and $D^*$ is the simulated data from the model. The Frobenius norm is used to measure the difference between the two datasets.
  
```{r parameters-setup-constants}

```

### Data
```{r data, include=FALSE }
## Row names for the data frames
nb_infecteds = c("0_infected", "1_infected", "2_infected", "3_infected", "4_infected", "5_infected")

## Infuenza A from 1977-1978
suppl2_7778 = data.frame(
    household_1 = c(66, 13, 0, 0, 0 , 0),
    household_2 = c(87, 14, 4, 0, 0, 0),
    household_3 = c(25, 15, 4, 4, 0, 0),
    household_4 = c(22, 9, 9, 3, 1, 0),
    household_5 = c(4, 4, 1, 1, 1, 0))
row.names(suppl2_7778) = nb_infecteds

## Influenza A from 1980-1981
suppl2_8081 = data.frame(
    household_1 = c(44, 10, 0, 0, 0 , 0),
    household_2 = c(62, 13, 9, 0, 0, 0),
    household_3 = c(47, 8, 2, 3, 0, 0),
    household_4 = c(38, 11, 7, 5, 1, 0),
    household_5 = c(9, 5, 3, 1, 0, 1))
row.names(suppl2_8081) = nb_infecteds


## Influenza B from 1975-1976
suppl3_7576 = data.frame(
    household_1 = c(9, 1, 0, 0, 0 , 0),
    household_2 = c(12, 6, 2, 0, 0, 0),
    household_3 = c(18, 6, 3, 1, 0, 0),
    household_4 = c(9, 4, 4, 3, 0, 0),
    household_5 = c(4, 3, 0, 2, 0, 0))
row.names(suppl3_7576) = nb_infecteds

suppl3_7879 = data.frame(
    household_1 = c(15, 11, 0, 0, 0 , 0),
    household_2 = c(12, 17, 21, 0, 0, 0),
    household_3 = c(4, 4, 4, 5, 0, 0))
row.names(suppl3_7879) = nb_infecteds


```



##  Functions for epidemic simulation
### Implementation of $w_{js}$ and distance function :
Below is the implementation of $w_{js}$, the distance function, and the outbreak simulation function.
```{r w_js-function, include=TRUE}
# Probability that j out of s susceptibles in a household become infected
w_js = function(j, s, q_c, q_h) {
    # w_0s
    if (j==0){ return (q_c^s) } 
    
    # w_jj for j = 1, 2, ..., s-1
    else if (j==s){
        sum_w_ij = 0
        
        for (i in 0:(j-1)){ 
          sum_w_ij = sum_w_ij + w_js(i, j, q_c, q_h)
          }
        return (1 - sum_w_ij)
    
    # w_js for j = 1, 2, ..., s-1, s different from j
    } else {
        comb = choose(s, j)
        w_jj = w_js(j, j, q_c, q_h)
        
        return (comb * w_jj * (q_c * q_h^j)^(s - j))
    }
}


# Distance function between observed data D_0 and simulated data D_star
distance = function(D_0, D_star) {
    # As D_0 = D1 U D2
    D1 = D_0$D1
    D2 = D_0$D2
    D_star1 = D_star$D1
    D_star2 = D_star$D2
    
    frob_norm1 = sqrt(sum((D1 - D_star1)^2))
    frob_norm2 = sqrt(sum((D2 - D_star2)^2))
    
    return (0.5 * frob_norm1 + 0.5 * frob_norm2)
}


```

### Outbreak simulation functions:

In the paper: 
- Supplementary Table 2 summarises the final household outcomes for two influenza A epidemics, in 1977-78 and 1980-81. For each epidemic, the columns correspond to households of a given size (1 to 5 individuals initially susceptible in the household), and the rows give the number of infected individuals in that household at the end of the outbreak (0 to 5). Each entry in the table is the count of households with that combination of household size and final number infected.

- Supplementary Table 3 has the same layout for two different epidemics: an influenza B outbreak in 1975-76 and an influenza A outbreak in 1978-79. Again, columns index household size (number of initially susceptible individuals per household), rows index the final number of infections in the household, and each cell reports how many households of that size experienced that number of infections in the corresponding epidemic.

In the next part, I will implement the outbreak simulation functions based on the above description.
Then I will simulate the outbreaks under different models and parameter settings.

```{r simulate-outbreak, include=TRUE}

```

# 3. Setup for ABC-SMC algorithm
## Model and prior setup
### Constants
```{r model-setup, include=FALSE}

# Number of candidate models M = 2 
# ------------------------------------
## For table 2 : we compared between 2 vs 4 parameters
## For table 3, we compared between 2 vs 3 parameters
M = 2 # since we compare between 2 models only

# ===================================================
# Model 1 = 2 parameters (shared qc, qh)
# Model 2 = 4 parameters (qc1, qh1, qc2, qh2)
# Model 3 = 3 parameters (qc1, qc2, qh)
n_params_by_model_t2 = c("1" = 2, "2" = 4)
n_params_by_model_t3 = c("1" = 2, "2" = 3)
```

### Prior distributionns
```{r priors-setup, include=FALSE}
# Prior for models
# here we consider only 2 models (M = 2)
prior_model = function(M) {
    return (sample(1:M, size=1, prob=rep(1/M, M)))
}

# Prior distribution for parameter given model m
# n_params_by_model: named vector of number of parameters per model for table 2 or table 3
prior_theta_parameters = function(m, n_params_by_model) {
    n_par = n_params_by_model[as.character(m)]
    return (runif(n_par, 0, 1))
}

```

## Helper functions : pertubation kernels
In the paper, the authors used a uniform perturbation kernel for parameters.
The kernel is centered at the previous particle, and the scale is set using the previous particles for the same model.

```{r pertubation-kernels, include=FALSE}
# Similar to the paper : KPt 

# theta_mat_model: matrix of particles and for T populations (N x T)
# where N is number of particles, T is number of parameters

# theta_prev: previous particle to perturb (vector of length N)
# Returns a new perturbed theta_new, and update the theta_mat_model accordingly.

# theta_star: vector (length d) = centre of the kernel (ancestor)
# theta_prev_mat_model: matrix of previous thetas for this model, N_model x d
perturb_theta <- function(theta_star, theta_prev_mat_model) {
  d <- length(theta_star)
  if (nrow(theta_prev_mat_model) == 0) {
    stop("No previous particles for this model")
  }

  # σ_k = 0.5 * (max - min) for each parameter coordinate k
  sigma = 0.5 * (apply(theta_prev_mat_model, 2, max) -
                  apply(theta_prev_mat_model, 2, min))
  sigma[sigma == 0] = 0.1  # avoid zero-width kernels

  # θ_new = θ_star + U(-σ_k, σ_k) component-wise
  theta_new <- theta_star + runif(d, -sigma, sigma)

  # keep parameters inside [0,1] (matches Uniform(0,1) prior support)
  theta_new[theta_new < 0] = 0
  theta_new[theta_new > 1] = 1

  theta_new
}



# Density of the kernel at theta_new given centre theta_prev.
kernel_theta_density = function(theta_new, theta_prev, theta_prev_mat_model) {
  d = length(theta_prev)
  
  sigma = 0.5 * (apply(theta_prev_mat_model, 2, max) -
                  apply(theta_prev_mat_model, 2, min))
  sigma[sigma == 0] = 0.1
  dens = 1
  for (p in seq_len(d)) {
    if (abs(theta_new[p] - theta_prev[p]) > sigma[p]) {
      return(0) # this is outside the support of the uniform kernel
    } else {
      dens = dens * (1 / (2 * sigma[p]))
    }
  }
  return(dens)
}
```


```{r abc-smc-function, include=FALSE}
abc_smc_influenza = function(D0,
                              epsilon_schedule,
                              n_params_by_model,
                              N_particles = N,
                              M_models = M) {

  # Initialization
  T_pop   = length(epsilon_schedule)
  max_par = max(n_params_by_model) # maximum number of parameters for all models

  # Save final results in a matrix / list
  models_mat  = matrix(NA_integer_, nrow = N_particles, ncol = T_pop)
  weights_mat = matrix(NA_real_,     nrow = N_particles, ncol = T_pop)
  thetas_list = vector("list", T_pop)  # each [[t]]: N x max_par matrix

  ## ----------------- t = 1: sample directly from priors -----------------
  eps1 = epsilon_schedule[1]
  theta_mat_t1 = matrix(NA_real_, nrow = N_particles, ncol = max_par)

  for (i in seq_len(N_particles)) {
    repeat {
      m_star = prior_model(M_models) # choose the model either 1 or 2
      theta_st = prior_theta_parameters(m_star, n_params_by_model = n_params_by_model) # Given the model, choose parameters from prior
      D_star = simulate_data(theta_st, m_star, D0, n_params_by_model) # simulate data
      d_val = distance(D0, D_star)

      if (d_val <= eps1) {
        models_mat[i, 1] = m_star
        theta_mat_t1[i, 1:length(theta_st)] = theta_st
        weights_mat[i, 1] = 1  # unnormalised
        break
      }
    }
  }

  # normalise weights at t = 1
  weights_mat[, 1] = weights_mat[, 1] / sum(weights_mat[, 1])
  thetas_list[[1]] = theta_mat_t1

  ## ----------------- t >= 2: SMC steps -----------------
  if (T_pop >= 2) {
    for (t in 2:T_pop) {
      # next population
      epsilon_t = epsilon_schedule[t]

      # extract the data from previous population
      theta_prev_mat = thetas_list[[t - 1]]   # vector of length N x max_par     
      model_prev     = models_mat[, t - 1] # vector of length N
      w_prev         = weights_mat[, t - 1] # vector of length N

      # even if we have different number of parameters per model,
      # we store all particles in a matrix of size N x max_par, padding with NAs 
      theta_mat_t = matrix(NA_real_, nrow = N_particles, ncol = max_par)
      model_t     = integer(N_particles)
      w_t         = numeric(N_particles)


      # Loop over particles
      for (i in seq_len(N_particles)) {
    
        repeat {
          # From the previous population, we have a sample of (model, theta) particles with weights.
          # We will sample from these to generate new particles for the current population.
          # using the weights from previous population.

          # 1/ Choose an ancestor index across all models
          ancestor_idx = sample(seq_len(N_particles), size = 1, prob = w_prev)
          m_star       = model_prev[ancestor_idx]

          # 2/ Work only with previous particles of this model
          idx_m        = which(model_prev == m_star)
          d_m          = n_params_by_model[as.character(m_star)]
          theta_prev_m = theta_prev_mat[idx_m, 1:d_m, drop = FALSE]
          w_prev_m     = w_prev[idx_m]

          # 3/ Choose which theta to perturb within this model
          j_idx        = sample(seq_along(idx_m), size = 1, prob = w_prev_m / sum(w_prev_m))
          theta_center = theta_prev_m[j_idx, ]

          # 4) Perturb parameters
          theta_st = perturb_theta(theta_center, theta_prev_m)

          # Simulate dataset and compute distance
          D_star = simulate_data(theta_st, m_star, D0, n_params_by_model)
          d_val  = distance(D0, D_star)
          


          if (d_val <= epsilon_t) {
           
            model_t[i] = m_star

            theta_full = rep(NA_real_, max_par)
            theta_full[1:length(theta_st)] = theta_st
            theta_mat_t[i, ] = theta_full

            break
          }
        } # repeat for particle i

        ## weight calculation MS3
        # importance of particle i at population t
        m_i = model_t[i]
        d_i = n_params_by_model[as.character(m_i)]
        theta_i_short = theta_mat_t[i, 1:d_i]

        # previous population restricted to same model
        idx_same_model = which(model_prev == m_i)
        theta_prev_m   = theta_prev_mat[idx_same_model, 1:d_i, drop = FALSE]
        w_prev_m       = w_prev[idx_same_model]

        kernel_vals = numeric(length(idx_same_model))
        for (k in seq_along(idx_same_model)) {
          theta_prev_k = theta_prev_m[k, ]
          kernel_vals[k] = kernel_theta_density(theta_i_short,
                                                 theta_prev_k,
                                                 theta_prev_m)
        }
        denom = sum(w_prev_m * kernel_vals)

        # Prior on m and theta|m are uniform on [0,1] so numerator is constant
        w_t[i] = ifelse(denom > 0, 1 / denom, 0)
      } # end i particle loop
    

      # normalise weights
      w_t = w_t / sum(w_t)

      models_mat[, t]  = model_t
      weights_mat[, t] = w_t
      thetas_list[[t]] = theta_mat_t
    } # end t population loop
  }

  return(list(
    models  = models_mat,
    thetas  = thetas_list,
    weights = weights_mat,
    epsilon = epsilon_schedule
  ))
}

```

```{r run-abc-smc-table2, include=FALSE}

D0_suppl2 = list(
  D1 = as.matrix(suppl2_7778),
  D2 = as.matrix(suppl2_8081)
)

epsilon_t = tolerance.sched.t2  # schedule for table 2

set.seed(123)
res_suppl2 = abc_smc_influenza(
  D0              = D0_suppl2,
  epsilon_schedule = epsilon_t,
  N_particles      = N,
  M_models         = M,
  n_params_by_model = n_params_by_model_suppl2
)

# Posterior model probabilities in the last population:
t_last = length(epsilon_t)
post_model_probs = tapply(res_suppl2$weights[, t_last],
                           res_suppl2$models[, t_last],
                           sum)
post_model_probs = post_model_probs / sum(post_model_probs)
post_model_probs

```

```{r summary-utilities, include=FALSE}
# Extract posterior particles at population t
get_posterior_particles = function(res, t = length(res$epsilon)) {
  theta_mat = res$thetas[[t]]
  colnames(theta_mat) = paste0("theta", seq_len(ncol(theta_mat)))
  data.frame(
    model  = res$models[, t],
    weight = res$weights[, t],
    theta_mat
  )
}

# Attach parameter names for the Supplementary Table 2 setting:
# model 1: shared q_c, q_h
# model 2: q_c1, q_h1, q_c2, q_h2
label_parameters_suppl2 = function(df) {
  df$q_c_shared = ifelse(df$model == 1, df$theta1, NA)
  df$q_h_shared = ifelse(df$model == 1, df$theta2, NA)
  df$q_c1       = ifelse(df$model == 2, df$theta1, NA)
  df$q_h1       = ifelse(df$model == 2, df$theta2, NA)
  df$q_c2       = ifelse(df$model == 2, df$theta3, NA)
  df$q_h2       = ifelse(df$model == 2, df$theta4, NA)
  df
}

# Posterior model probabilities at population t
posterior_model_probs_fun = function(res, t = length(res$epsilon)) {
  df = data.frame(
    model  = res$models[, t],
    weight = res$weights[, t]
  )
  agg = aggregate(weight ~ model, data = df, FUN = sum)
  names(agg)[names(agg) == "weight"] = "prob"
  agg$prob = agg$prob / sum(agg$prob)
  agg
}

# Weighted quantiles (for credible intervals)
weighted_quantile = function(x, w, probs = c(0.025, 0.5, 0.975)) {
  ord = order(x)
  x = x[ord]; w = w[ord]
  w = w / sum(w)
  cw = cumsum(w)
  sapply(probs, function(p) {
    idx = which(cw >= p)[1]
    x[idx]
  })
}
```

```{r summary-utilities-helpers, include=FALSE}
summarise_param_suppl2 = function(df, param, model_value) {
  sub = subset(df, model == model_value & !is.na(.data[[param]]))
  x = sub[[param]]
  w = sub$weight
  c(
    mean = sum(x * w) / sum(w),
    weighted_quantile(x, w)
  )
}
```

## 3. Summaries for Supplementary Table 2 (for Fig. 3a descriptive stats)


```{r summarise-table2, echo=TRUE}
t_last = length(res_suppl2$epsilon)

# Posterior particles with named parameters
post_df_suppl2 = get_posterior_particles(res_suppl2, t = t_last)
post_df_suppl2 = label_parameters_suppl2(post_df_suppl2)

# Posterior model probabilities (this is Fig. 3b numerically)
posterior_model_probs_fun(res_suppl2)

# Summaries for the four-parameter model (model 2)
post4 = subset(post_df_suppl2, model == 2)

params = c("q_c1", "q_h1", "q_c2", "q_h2")
summaries = lapply(params, function(p) {
  x = post4[[p]]
  w = post4$weight
  c(mean = sum(x * w) / sum(w),
    weighted_quantile(x, w))
})
names(summaries) = params
summaries
```


```{r load-plot-libs, message=FALSE}
library(ggplot2)
library(dplyr)
library(tidyr)
```


```{r fig3a-replot, fig.height=4, fig.width=6}
# Restrict to four-parameter model (model 2)
post4 = subset(post_df_suppl2, model == 2)

# Resample from weighted particles to get an unweighted sample
set.seed(123)
n_sample = 10
idx = sample(seq_len(nrow(post4)), size = n_sample,
              replace = TRUE, prob = post4$weight)
samp4 = post4[idx, ]

# Long format: q_c and q_h, for each outbreak
qc_long = rbind(
  data.frame(outbreak = "1977-1978", param = "q_c", value = samp4$q_c1),
  data.frame(outbreak = "1980-1981", param = "q_c", value = samp4$q_c2)
)

qh_long = rbind(
  data.frame(outbreak = "1977-1978", param = "q_h", value = samp4$q_h1),
  data.frame(outbreak = "1980-1981", param = "q_h", value = samp4$q_h2)
)

# Posterior of q_c (panel corresponding to Fig. 3a, first row)
p_qc = ggplot(qc_long, aes(x = value, colour = outbreak, fill = outbreak)) +
  geom_density(alpha = 0.2) +
  labs(x = expression(q[c]), y = "Posterior density",
       title = "Posterior of q[c] (four-parameter model, Supplementary Table 2)") +
  xlim(0, 1)

# Posterior of q_h (panel corresponding to Fig. 3a, second row)
p_qh = ggplot(qh_long, aes(x = value, colour = outbreak, fill = outbreak)) +
  geom_density(alpha = 0.2) +
  labs(x = expression(q[h]), y = "Posterior density",
       title = "Posterior of q[h] (four-parameter model, Supplementary Table 2)") +
  xlim(0, 1)





```

## 5. Reproducing Fig. 3b (posterior model probabilities \(P(m \mid D_0)\))

```{r fig3b-replot, fig.height=3, fig.width=5}
post_model_probs2 = posterior_model_probs_fun(res_suppl2)

post_model_probs2$model = factor(
  post_model_probs2$model,
  levels = c(1, 2),
  labels = c("Model 1 (2 parameters)", "Model 2 (4 parameters)")
)

ggplot(post_model_probs2, aes(x = model, y = prob)) +
  geom_col(width = 0.5) +
  ylim(0, 1) +
  labs(x = "Model",
       y = expression(P(m ~ "|" ~ D[0])),
       title = "Posterior model probabilities (Supplementary Table 2)")
```

This is the analogue of Fig. 3b: a bar for the 2-parameter model vs the 4-parameter model. Toni & Stumpf report \(\Pr(m=1 \mid D_0) \approx 0.98\) as a median over 10 runs; your single run will be in that neighbourhood but not exactly identical.   



## 6. How to extend to Fig. 3c-d (Seattle outbreaks)

Once you:

- run an analogous ABC-SMC for **Supplementary Table 3** (either:
  - four-parameter model only, to get Fig. 3c, and
  - model 1 = three parameters \((q_{c1}, q_{c2}, q_h)\) vs model 2 = four parameters, for Fig. 3d),   

you can reuse **exactly** the same utilities:

- `get_posterior_particles()` → label parameters appropriately (you'll want a `label_parameters_suppl3()` for the 3-parameter/4-parameter semantics).
- `posterior_model_probs_fun()` → bar plot for Fig. 3d.
- The density plotting pattern from `fig3a-replot` → two density panels for \(q_c\) and \(q_h\) for the four-parameter model (Fig. 3c).

But for the current code you pasted (set up for Suppl. Table 2, 2- vs 4-parameter models), the chunks above give you:

- numerical posterior summaries,  
- a clean re-plot of Fig. 3a (for \(q_c\) and \(q_h\)), and  
- a re-plot of Fig. 3b (posterior model probs).
